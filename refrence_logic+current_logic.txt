"""
Enhanced Generic Bank Statement Parser
Handles multiple bank formats, various header structures, and flexible data extraction
"""
from typing import List, Dict, Optional, Union, Tuple, Any
from datetime import datetime
import io
import re
import logging

import pandas as pd
import pdfplumber
from fastapi import HTTPException, status

from app.schemas import StatementItem

# Optional OCR fallback
import pytesseract
from pdf2image import convert_from_bytes

# Configure Tesseract
DEFAULT_TESSERACT_CMD = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
pytesseract.pytesseract.tesseract_cmd = pytesseract.pytesseract.tesseract_cmd or DEFAULT_TESSERACT_CMD

logger = logging.getLogger(__name__)

# Enhanced header aliases with priority-based mapping
HEADER_ALIASES: Dict[str, Dict[str, int]] = {
    "date": {
        # Primary date fields (higher priority)
        "tran date": 10, "transaction date": 10, "txn date": 10,
        "date": 9, "posting date": 9, "entry date": 9,
        # Secondary date fields
        "value date": 5, "value dt": 5, "date (value date)": 5,
        "date(value date)": 5, "date value": 5, "booking date": 5,
        "settlement date": 4, "trade date": 4, "deal date": 4,
        "post date": 4, "process date": 4, "accounting date": 4
    },
    "narration": {
        "particulars": 10, "description": 10, "narration": 10,
        "transaction details": 9, "details": 9, "transaction particulars": 8,
        "narrative": 8, "payee": 7, "merchant/payee information": 7,
        "remarks": 6, "memo": 6, "transaction description": 6,
        "payee details": 5, "beneficiary": 5, "merchant name": 5
    },
    "instrument": {
        "tr mode instrument": 10, "tr. mode instrument": 10,
        "mode instrument": 9, "instrument": 9, "payment mode": 8,
        "transfer mode": 8, "payment instrument": 7, "txn mode": 7,
        "transaction method": 6, "transaction type": 6, "channel": 5
    },
    "ref_no": {
        "ref/chequeno.": 10, "ref/cheque no.": 10, "ref no": 10,
        "cheque no.": 9, "cheque number": 9, "chq no": 9,
        "chq./ref no.": 9, "ref. no": 9, "reference no": 8,
        "transaction id": 8, "utr no": 8, "utr/reference no": 7,
        "voucher no": 6, "batch no": 5, "advice no": 5, "Refchequeno": 10, "Ref/Cheque No.": 10
    },
    "debit": {
        "debit": 10, "debit amt.": 10, "debit amount": 10,
        "withdrawal": 9, "withdrawn": 9, "dr": 9, "dr amt": 9,
        "amount withdrawn": 8, "withdrawal amount": 8, "paid out": 7,
        "payments out": 7, "money out": 7, "outflow": 6, "charge": 5
    },
    "credit": {
        "credit": 10, "credit amt.": 10, "credit amount": 10,
        "deposit": 9, "cr": 9, "cr amt": 9, "amount deposited": 8,
        "paid in": 7, "payments in": 7, "money in": 7,
        "inflow": 6, "receipt": 6, "collection": 5
    },
    "balance": {
        "balance": 10, "running balance": 9, "available balance": 9,
        "closing balance": 8, "new balance": 8, "current balance": 7,
        "ledger balance": 6, "account balance": 6, "book balance": 5
    }
}

def map_headers_with_priority(raw_headers: List[str]) -> Dict[str, str]:
    mapping: Dict[str, str] = {}
    field_assignments: Dict[str, Tuple[str, int]] = {}

    for raw_header in raw_headers:
        if not raw_header or pd.isna(raw_header):
            continue

        norm = normalize_header_name(raw_header)
        if not norm:
            continue

        best_field = None
        best_priority = 0

        for field, aliases in HEADER_ALIASES.items():
            if norm in aliases:
                priority = aliases[norm]
                if priority > best_priority:
                    best_field = field
                    best_priority = priority

        if best_field:
            if best_field in field_assignments:
                existing_header, existing_priority = field_assignments[best_field]
                if best_priority > existing_priority:
                    if mapping.get(existing_header) == best_field:
                        del mapping[existing_header]
                    mapping[raw_header] = best_field
                    field_assignments[best_field] = (raw_header, best_priority)
            else:
                mapping[raw_header] = best_field
                field_assignments[best_field] = (raw_header, best_priority)

    logger.debug("ðŸ—º Header mapping: %s", mapping)
    return mapping

def normalize_header_name(raw: str) -> str:
    if not raw or pd.isna(raw):
        return ""

    h = str(raw).strip().lower()
    h = re.sub(r"\s*\([^)]*\)", "", h)
    h = re.sub(r"[^\w\s.]", " ", h)
    h = re.sub(r"\s+", " ", h)
    h = h.strip(". ")
    return h

def parse_date_string(date_input: Union[str, pd.Series, Any]) -> Optional[datetime]:
    """Enhanced date parsing with better type handling."""
    # Handle pandas Series or other non-string types
    if isinstance(date_input, pd.Series):
        if date_input.empty:
            return None
        date_str = str(date_input.iloc[0]) if len(date_input) > 0 else ""
    else:
        date_str = str(date_input) if date_input is not None else ""
    
    if not date_str or date_str.lower() in ['nan', 'none', '', 'nat']:
        return None
    
    # Clean the date string
    date_str = date_str.strip()
    if not date_str:
        return None
    
    # Remove any extra content after the date
    date_parts = date_str.split()
    if date_parts:
        date_str = date_parts[0]
    
    # Comprehensive date formats
    date_formats = [
        "%d/%m/%Y", "%d-%m-%Y", "%d/%m/%y", "%d-%m-%y",
        "%d-%b-%Y", "%d-%b-%y", "%d %b %Y", "%d %b %y",
        "%Y-%m-%d", "%Y/%m/%d", "%m/%d/%Y", "%m-%d-%Y",
        "%d.%m.%Y", "%d.%m.%y", "%Y.%m.%d",
        "%d %B %Y", "%d-%B-%Y", "%B %d, %Y"
    ]
    
    for fmt in date_formats:
        try:
            parsed_date = datetime.strptime(date_str, fmt)
            logger.debug("âœ… Parsed date '%s' with format '%s'", date_str, fmt)
            return parsed_date
        except ValueError:
            continue
    
    logger.debug("âŒ Failed to parse date: '%s'", date_str)
    return None

def safe_numeric_conversion(value: Any) -> Optional[float]:
    """Safely convert various types to numeric values."""
    if value is None or pd.isna(value):
        return None
    
    # Handle pandas Series
    if isinstance(value, pd.Series):
        if value.empty:
            return None
        value = value.iloc[0] if len(value) > 0 else None
        if value is None or pd.isna(value):
            return None
    
    # Convert to string and clean
    str_val = str(value).strip()
    if not str_val or str_val.lower() in ['nan', 'none', '', '0.0', '0']:
        return None
    
    # Remove common formatting
    str_val = str_val.replace(',', '').replace('â‚¹', '').replace('$', '')
    str_val = re.sub(r'[^\d.-]', '', str_val)
    
    if not str_val:
        return None
    
    try:
        return float(str_val)
    except ValueError:
        logger.debug("âŒ Failed to convert to number: '%s'", str_val)
        return None

def enhanced_reference_extraction(row_dict: Dict[str, Any], description: str) -> Optional[str]:
    """Enhanced reference number extraction with multiple strategies."""
    # Strategy 1: Direct ref_no column
    ref_no = row_dict.get('ref_no')
    if ref_no and not pd.isna(ref_no):
        ref_str = str(ref_no).strip()
        if ref_str and ref_str.lower() not in ['nan', 'none', '']:
            return ref_str
    
    # Strategy 2: Search all columns for reference patterns
    all_text = []
    for key, value in row_dict.items():
        if value and not pd.isna(value):
            text = str(value).strip()
            if text and text.lower() not in ['nan', 'none']:
                all_text.append(text)
    
    combined_text = " ".join(all_text + [description])
    
    # Reference patterns (ordered by specificity)
    patterns = [
        # UPI/Payment specific
        r"(?:UPI|IMPS|NEFT|RTGS)[/\-:]([A-Z0-9]{8,})",
        r"(?:UPI|IMPS)[/\-:](?:[A-Z]{2,4}[/\-:])?(\d{10,})",
        # Transaction references
        r"(?:REF|UTR|TXN)[/\-:]([A-Z0-9]{6,})",
        # Cheque numbers
        r"(?:CHQ|CHEQUE)[/\-:](\d{6,})",
        # Generic long numbers
        r"\b(\d{12,})\b",
        r"\b([A-Z]{3,}\d{8,})\b",
        r"\b(\d{10,})\b"
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, combined_text, re.IGNORECASE)
        if matches:
            ref = matches[0]
            logger.debug("âœ… Found reference with pattern '%s': %s", pattern, ref)
            return ref
    
    logger.debug("âŒ No reference found in: %s", combined_text[:100])
    return None

def parse_flexible_rows(df: pd.DataFrame) -> List[StatementItem]:
    """Parse DataFrame rows with enhanced flexibility."""
    items: List[StatementItem] = []

    if df.empty or df.shape[1] < 3:
        logger.debug("âš  Empty or too few columns in DataFrame")
        return items

    logger.debug("ðŸ”„ Starting to parse %d rows", len(df))
    logger.debug("ðŸ“Š Available columns: %s", list(df.columns))

    for idx, row in df.iterrows():
        try:
            logger.debug("ðŸ§¾ Processing row %d", idx)

            row_dict = row.to_dict()

            # Date extraction
            date = None
            date_columns = [col for col in df.columns if 'date' in col.lower()]
            for date_col in date_columns:
                if date_col in row_dict:
                    date = parse_date_string(row_dict[date_col])
                    if date:
                        logger.debug("âœ… Found date in column '%s': %s", date_col, date)
                        break

            if not date:
                logger.debug("âš  Skipping row %d - no valid date found", idx)
                continue

            # Amount extraction
            debit = safe_numeric_conversion(row_dict.get('debit'))
            credit = safe_numeric_conversion(row_dict.get('credit'))
            balance = safe_numeric_conversion(row_dict.get('balance'))

            # Fallback if no amount
            if debit is None and credit is None:
                logger.debug("ðŸ” No debit/credit found, searching all columns")
                for col, val in row_dict.items():
                    if any(kw in col.lower() for kw in ['date', 'narration', 'description', 'particulars']):
                        continue

                    num_val = safe_numeric_conversion(val)
                    if num_val is not None and num_val != 0:
                        col_lower = col.lower()
                        desc_text = str(row_dict.get('narration', '') or row_dict.get('description', '') or row_dict.get('particulars', '')).upper()

                        if 'debit' in col_lower:
                            debit = abs(num_val)
                        elif 'credit' in col_lower:
                            credit = abs(num_val)
                        elif 'dr' in col_lower and 'cr' not in col_lower:
                            debit = abs(num_val)
                        elif 'cr' in col_lower and 'dr' not in col_lower:
                            credit = abs(num_val)
                        elif 'TO ' in desc_text or 'PAID TO' in desc_text or ' DR ' in desc_text:
                            debit = abs(num_val)
                        elif ' CR ' in desc_text:
                            credit = abs(num_val)
                        else:
                            continue

                        logger.debug("ðŸ’¡ Found amount %s in column '%s' (classified as %s)",
                                     num_val, col, 'debit' if debit else 'credit')
                        break

            if debit is None and credit is None:
                logger.debug("âš  Skipping row %d - no amount found", idx)
                continue

            amount = credit if credit else -abs(debit)

            # Build description
            desc_parts = []
            for field in ['instrument', 'narration', 'description', 'particulars']:
                if field in row_dict and row_dict[field] and not pd.isna(row_dict[field]):
                    text = str(row_dict[field]).strip()
                    text = " ".join(text.splitlines())
                    if text and text.lower() not in ['nan', 'none']:
                        desc_parts.append(text)

            description = " - ".join(desc_parts) if desc_parts else "Transaction"

            # Filtered row_dict for reference extraction (only narration fields)
            ref_fields = ['instrument', 'narration', 'description', 'particulars']
            ref_dict = {k: row_dict[k] for k in ref_fields if k in row_dict}

            reference = enhanced_reference_extraction(ref_dict, description)

            item = StatementItem(
                date=date,
                description=description,
                amount=amount,
                balance=balance,
                ref_no=reference,
                debit=debit,
                credit=credit
            )
            items.append(item)

            logger.debug("âœ… Successfully parsed row %d: date=%s, amount=%s, desc=%s",
                         idx, date.strftime('%Y-%m-%d'), amount, description[:50])

        except Exception as e:
            logger.error("âŒ Error processing row %d: %s", idx, str(e))
            continue

    logger.debug("âœ… Successfully parsed %d items from %d rows", len(items), len(df))
    return items

def clean_and_validate_row(row: List[Any], expected_cols: int) -> Optional[List[str]]:
    """Enhanced row cleaning and validation."""
    if not row:
        return None
    
    # Convert all values to strings and clean
    cleaned = []
    for cell in row:
        if cell is None or pd.isna(cell):
            cleaned.append('')
        else:
            cell_str = str(cell).strip()
            # Clean multi-line content
            cell_str = " ".join(cell_str.splitlines())
            cleaned.append(cell_str)
    
    # Skip completely empty rows
    if not any(cleaned):
        return None
    
    # Skip header-like rows that appear in data
    row_text = ' '.join(cleaned).lower()
    skip_patterns = [
        r'^opening balance$',
        r'^closing balance$',
        r'statement.*period',
        r'account.*number',
        r'branch.*name'
    ]
    
    for pattern in skip_patterns:
        if re.search(pattern, row_text) and not any(word in row_text for word in ['neft', 'rtgs', 'upi', 'transfer']):
            logger.debug("ðŸš« Skipping header-like row: %s", row_text[:50])
            return None
    
    # Ensure minimum data quality
    non_empty_count = sum(1 for cell in cleaned if cell)
    if non_empty_count < 2:
        return None
    
    # Pad or truncate to expected length
    while len(cleaned) < expected_cols:
        cleaned.append('')
    
    return cleaned[:expected_cols]

def find_header_row_enhanced(table: List[List[Any]], min_confidence: float = 0.6) -> Optional[int]:
    """Enhanced header detection with confidence scoring."""
    if not table:
        return None
    
    all_aliases = set()
    for field_aliases in HEADER_ALIASES.values():
        all_aliases.update(field_aliases.keys())
    
    best_row_idx = None
    best_score = 0
    
    for idx, row in enumerate(table):
        if not row or len(row) < 3:  # Need at least 3 columns
            continue
        
        # Calculate confidence score
        total_cols = len([cell for cell in row if cell and str(cell).strip()])
        if total_cols == 0:
            continue
        
        matches = 0
        for cell in row:
            if cell and not pd.isna(cell):
                normalized = normalize_header_name(str(cell))
                if normalized in all_aliases:
                    matches += 1
        
        confidence = matches / total_cols
        
        if confidence >= min_confidence and confidence > best_score:
            best_score = confidence
            best_row_idx = idx
            
        logger.debug("ðŸ“Š Row %d: %d matches out of %d columns (confidence: %.2f)", 
                    idx, matches, total_cols, confidence)
    
    if best_row_idx is not None:
        logger.debug("ðŸŽ¯ Best header row: %d (confidence: %.2f)", best_row_idx, best_score)
    
    return best_row_idx

def extract_tables_enhanced(file_bytes: bytes) -> List[Dict]:
    """Enhanced table extraction with better error handling."""
    transactions: List[Dict] = []
    last_header: List[str] = []
    last_mapping: Dict[str, str] = {}
    
    # Patterns to filter out footer/header noise
    NOISE_PATTERNS = [
        re.compile(r"State Bank of India|printstatement|Page \d+", re.IGNORECASE),
        re.compile(r"^\s*$"),  # Empty lines
        re.compile(r"^-+$"),   # Separator lines
    ]
    
    try:
        with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
            logger.debug("ðŸ“„ Processing PDF with %d pages", len(pdf.pages))
            
            for page_num, page in enumerate(pdf.pages, 1):
                logger.debug("ðŸ“„ Processing page %d", page_num)
                
                # Extract and clean text
                raw_text = page.extract_text() or ""
                clean_lines = []
                for line in raw_text.splitlines():
                    if not any(pattern.search(line) for pattern in NOISE_PATTERNS):
                        clean_lines.append(line)
                
                # Override page text with cleaned version
                if hasattr(page, '_text'):
                    page._text = "\n".join(clean_lines)
                
                # Extract tables
                try:
                    tables = page.extract_tables() or []
                    logger.debug("ðŸ“‹ Found %d tables on page %d", len(tables), page_num)
                    
                    for table_idx, table in enumerate(tables, 1):
                        if not table or len(table) < 1:
                            continue
                        
                        logger.debug("ðŸ” Processing table %d (page %d) with %d rows", 
                                   table_idx, page_num, len(table))
                        
                        # Try to find header
                        header_idx = find_header_row_enhanced(table)
                        current_header = []
                        current_mapping = {}
                        data_start_idx = 0
                        
                        if header_idx is not None:
                            # Found header in this table
                            current_header = [str(cell) if cell else '' for cell in table[header_idx]]
                            current_mapping = map_headers_with_priority(current_header)
                            last_header = current_header
                            last_mapping = current_mapping
                            data_start_idx = header_idx + 1
                            logger.debug("âœ… Found header at row %d: %s", header_idx, current_header)
                            
                        elif last_header:
                            # Use cached header from previous table/page
                            current_header = last_header
                            current_mapping = last_mapping
                            data_start_idx = 0
                            logger.debug("ðŸ” Using cached header: %s", current_header)
                            
                        else:
                            logger.debug("âš  No header found for table %d on page %d", table_idx, page_num)
                            continue
                        
                        if not current_mapping:
                            logger.debug("âš  No field mapping for table %d on page %d", table_idx, page_num)
                            continue
                        
                        # Extract and clean data rows
                        data_rows = []
                        for row_idx in range(data_start_idx, len(table)):
                            cleaned_row = clean_and_validate_row(table[row_idx], len(current_header))
                            if cleaned_row:
                                data_rows.append(cleaned_row)
                        
                        if not data_rows:
                            logger.debug("âš  No valid data rows in table %d on page %d", table_idx, page_num)
                            continue
                        
                        # Create DataFrame and parse
                        try:
                            df = pd.DataFrame(data_rows, columns=current_header)
                            df = df.rename(columns=current_mapping)
                            
                            logger.debug("ðŸ“Š Created DataFrame with columns: %s", list(df.columns))
                            
                            parsed_items = parse_flexible_rows(df)
                            for item in parsed_items:
                                transactions.append(vars(item))
                            
                            logger.debug("âœ… Parsed %d transactions from table %d on page %d", 
                                       len(parsed_items), table_idx, page_num)
                            
                        except Exception as e:
                            logger.error("âŒ Error creating DataFrame for table %d on page %d: %s", 
                                       table_idx, page_num, str(e))
                            continue
                            
                except Exception as e:
                    logger.error("âŒ Error extracting tables from page %d: %s", page_num, str(e))
                    continue
    
    except Exception as e:
        logger.error("âŒ Critical error in table extraction: %s", str(e))
        return []
    
    logger.debug("âœ… Total transactions extracted: %d", len(transactions))
    return transactions

# Update main parsing functions
def parse_file(file_bytes: bytes, filename: str) -> List[StatementItem]:
    """Enhanced main parsing function."""
    ext = filename.lower().rsplit('.', 1)[-1]
    
    if ext == 'pdf':
        records = extract_tables_enhanced(file_bytes)
        if not records:
            raise HTTPException(status.HTTP_400_BAD_REQUEST, 
                              "No transactions could be parsed from the PDF. Please ensure the file contains a valid bank statement.")
    elif ext == 'csv':
        try:
            df = pd.read_csv(io.BytesIO(file_bytes))
            mapping = map_headers_with_priority(list(df.columns))
            df = df.rename(columns=mapping)
            records = [vars(item) for item in parse_flexible_rows(df)]
        except Exception as e:
            logger.error("âŒ Error parsing CSV: %s", str(e))
            raise HTTPException(status.HTTP_400_BAD_REQUEST, f"Error parsing CSV file: {str(e)}")
    else:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, f"Unsupported file type: .{ext}")
    
    return [StatementItem(**record) for record in records]

# Service class remains the same but uses enhanced functions
class ParserService:
    """Enhanced service wrapper."""
    
    @staticmethod
    def extract_text_bytes(b: bytes) -> str:
        try:
            with pdfplumber.open(io.BytesIO(b)) as pdf:
                text = '\n'.join(page.extract_text() or '' for page in pdf.pages)
            if not text.strip():
                logger.debug("ðŸ“ƒ No text layer; using OCR")
                imgs = convert_from_bytes(b)
                text = '\n'.join(pytesseract.image_to_string(img) for img in imgs)
            return text
        except Exception as e:
            logger.error("âŒ Error extracting text: %s", str(e))
            return ''
    
    @staticmethod
    def extract_text(b: bytes) -> str:
        return ParserService.extract_text_bytes(b)
    
    @staticmethod
    def parse_file_bytes(b: bytes, f: str) -> List[StatementItem]:
        return parse_file(b, f)
    
    @staticmethod
    def parse_file(b: bytes, f: str) -> List[StatementItem]:
        return parse_file(b, f)